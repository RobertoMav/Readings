Agents:
	1. ReAct: https://arxiv.org/pdf/2210.03629 (PRINTED)
	2. Reflexion: https://arxiv.org/pdf/2303.11366 (PRINTED)
	3. What are Tools anyway: https://arxiv.org/pdf/2403.15452 (PRINTED)
	4. Mixture-of-Agents Enhances Large Language Model Capabilities: https://arxiv.org/pdf/2406.04692
	5. Agents that Matter: https://arxiv.org/pdf/2407.01502

ML/Math/RL:
	1. KAN: (Printed)
	2. How to avoid ML pitfalls: https://arxiv.org/pdf/2108.02497 (PRINTED)
	3. PPO: https://arxiv.org/pdf/1707.06347 (PRINTED)
	4. Transcendence: Generative Models Can Outperform The Experts That Train Them: https://arxiv.org/pdf/2406.11741v1 (PRINTED)
	5. Just How Flexible are Neural Networks in Practice: https://arxiv.org/pdf/2406.11463
	6. Large Minibatch SGD: https://arxiv.org/pdf/1706.02677
	7. DONâ€™T DECAY THE LEARNING RATE, INCREASE THE BATCH SIZE: https://arxiv.org/pdf/1711.00489
	8. Emergence in Non-Neural Models: https://arxiv.org/pdf/2407.20199
	9. Grokked Transformers are Implicit Reasoners: https://arxiv.org/pdf/2405.15071v2
	10. Train longer, generalize better: https://proceedings.neurips.cc/paper_files/paper/2017/file/a5e0ff62be0b08456fc7f1e88812af3d-Paper.pdf
	11. Intelligence at the Edge of Chaos: https://www.arxiv.org/pdf/2410.02536
	12. Shortcut Learning in Deep Neural Networks: https://arxiv.org/pdf/2004.07780
	13: Surge Phenomenon in Optimal Learning Rate and Batch Size Scaling: https://arxiv.org/pdf/2405.14578
	14: Deep Reinforcement Learning that Matters: https://arxiv.org/pdf/1709.06560

LLM:
	1. AlphaMath: https://arxiv.org/pdf/2405.03553 (PRINTED)
	2. Are Emergent Abilities of Large Language Models a Mirage?: https://arxiv.org/pdf/2304.15004 (PRINTED)
	3. Many-Shot In-Context Learning: https://arxiv.org/pdf/2404.11018 (PRINTED)
	4. Parameter Efficient Fine Tuning: A Comprehensive Analysis Across Applications: https://arxiv.org/pdf/2404.13506
	5. Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?: https://arxiv.org/pdf/2405.05904 (PRINTED)
	6. SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales: https://arxiv.org/pdf/2405.2097 (PRINTED)
	7. The Remarkable Robustness of LLMs: Stages of Inference: https://arxiv.org/pdf/2406.19384
	8. DOLA: DECODING BY CONTRASTING LAYERS IMPROVES FACTUALITY IN LARGE LANGUAGE MODELS: https://arxiv.org/pdf/2309.03883
	9. AGENTLESS: Demystifying LLM-based Software Engineering Agents: https://arxiv.org/pdf/2407.01489
	10. SURVEY OF PROMPT ENGINEERING METHODS: https://arxiv.org/pdf/2407.12994
	11. CHATGPT HAS A MIND: https://www.arxiv.org/pdf/2407.11015
	12. Large language models surpass human experts in predicting neuroscience results: file:///Users/rmartin0/Downloads/s41562-024-02046-9.pdf
	13. LLM as a Judge - a Survey: https://arxiv.org/pdf/2411.15594
	14: Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks: https://arxiv.org/pdf/2406.02550
	15: Learning Formal Mathematics From Intrinsic Motivation: https://arxiv.org/pdf/2407.00695
	16. Large Language Diffusion Models: https://arxiv.org/pdf/2502.09992

Tokenizers:
	1. MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers: https://arxiv.org/pdf/2305.07185
	2. Learning to Compress Prompts with Gist Tokens: https://arxiv.org/pdf/2304.08467

Compute:
	1. Scaling Laws for Neural Language Models: https://arxiv.org/pdf/2001.08361 (PRINTED)
	2. Training Compute-Optimal Large Language Models: https://arxiv.org/pdf/2203.15556 (PRINTED)
	3. Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automaton: https://arxiv.org/pdf/1405.6903 (PRINTED)
	4. Neural Turing Machines: https://arxiv.org/pdf/1410.5401 (PRINTED)

Double Descent/Learning:
	1. Reconciling modern machine learning practice and the bias-variance trade-off: https://arxiv.org/pdf/1812.11118
	2. Deep double descent: where bigger models and more data hurt: https://iopscience.iop.org/article/10.1088/1742-5468/ac3a74/pdf
	3. Do We Need Zero Training Loss After Achieving Zero Training Error?: https://arxiv.org/pdf/2002.08709
	4. A Universal Law of Robustness via Isoperimetry (Overparametrized models = smooth): https://dl.acm.org/doi/pdf/10.1145/3578580
	5. Deep Double Descent via Smooth Interpolation: https://arxiv.org/pdf/2209.10080
	6. Multi-scale Feature Learning Dynamics: Insights for Double Descent: https://proceedings.mlr.press/v162/pezeshki22a/pezeshki22a.pdf
	7. Do we really need a new theory to understand over-parameterization?: https://www.sciencedirect.com/science/article/pii/S0925231223003508
	8. GROKKING AT THE EDGE OF NUMERICAL STABILITY: https://arxiv.org/pdf/2501.04697